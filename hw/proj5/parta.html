<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>CS180: Intro to Computer Vision and Computational Photography</title>
    <link rel="StyleSheet" href="../style.css" type="text/css" media="all" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>

    <script>
      hljs.initHighlightingOnLoad();
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true
          }
        };
      </script>

    <script>
      hljs.initHighlightingOnLoad();
    </script>
        
    <style>
      code {
            background-color: #f4f4f4;
            padding: 5px;
            border-radius: 5px;
        }
        .image-container {
  display: flex;
  justify-content: center;
  align-items: flex-start;
  gap: 20px;
}

        .image-container {
  display: flex;
  justify-content: center;
  align-items: flex-start;
  gap: 20px;
}

.image-container > div {
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 30%;
  max-width: 200px;
}

.image-container img {
  width: 100%;
  height: auto;
}

.image-container p {
  text-align: center;
  margin-top: 10px;
}
      /* Two image containers */
      .column {
        float: left;
        width: 45%;
        padding: 5px;
      }

      /* Clear floats after image containers */
      .row::after {
        content: "";
        clear: both;
        display: table;
      }
    </style>
  </head>
  <body data-new-gr-c-s-check-loaded="14.1027.0" data-gr-ext-installed="">
    <h1>
      <img src="assets/ucbseal.png" alt="berkeley logo" height="75" width="75" />Programming Project #5 (<tt>proj5</tt>)<br />
      <a href="https://cal-cs180.github.io/fa23/index.html">CS180: Intro to Computer Vision and Computational Photography </a>
    </h1>
    <div class="image-container">
      <div>
        <img src="assets/reading.png" alt="Lion">
        <p>reading between the lions</p>
      </div>
      <div>
        <img src="assets/baby.png" alt="Baby">
        <p>bear with me</p>
      </div>
      <div>
        <img src="assets/skull.png" alt="Skull">
        <p>a lithograph of a skull</p>
      </div>
    </div>
    <br />
    <h1 style="text-align: center">Part A: The Power of Diffusion Models!</h1>
    <h3 style="text-align: center;">The first part of a <a href="./index.html">larger project</a>.</h3>
    <h2 style="text-align: center">
      <b style="color: red;">Due: 11/06/24 11:59pm</b>
    </h2>
    <h4 style="text-align: center">
      <b>We recommend using GPUs from <a href="https://colab.research.google.com/">Colab</a> to finish this project!</b>
    </h4>

    <h2>Overview</h2>
    <p>In part A you will use an off-the-shelf, pretrained text-to-image diffusion model to generate images from text prompts. Instructions can be found below and in the <a href="./parta.html">provided notebook</a>.</p>

    <p>Because part A is simply to get your feet wet with pre-trained diffusion models, all deliverables should be completed in <a href="./parta.html">this notebook</a>. You will still submit a webpage with your results.</p>

    <h1> Part 0: Setup</h1>
    <h3>Gaining Access to DeepFloyd</h3>
  <p class="text">
    We are going to use the <a href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if">DeepFloyd IF</a> diffusion model. DeepFloyd is a two stage model trained by Stability AI. The first stage produces images of size $64 \times 64$ and the second stage takes the outputs of the first stage and generates images of size $256 \times 256$. Before using DeepFloyd, you must accept its usage conditions. To do so:
  </p>
  <ol>
    <li>Make a <a href="https://huggingface.co/join">Hugging Face account</a> and log in.</li>
    <li>Accept the license on the model card of <a href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0">DeepFloyd/IF-I-XL-v1.0</a>. Accepting the license on the stage I model card will auto accept for the other IF models.</li>
    <li>Log in locally by entering your <a href="https://huggingface.co/docs/hub/security-tokens#what-are-user-access-tokens">Hugging Face Hub access token</a> below. You should be able to find and create tokens <a href="https://huggingface.co/settings/tokens">here</a>.</li>
  </ol>
  <b>Deliverable: </b> 
  <ul>
    <li>Display the output of the model when run with the provided text embeddings. Reflect on the quality of the outputs and their relationships to the text prompts.</li>
  </ul>
  <h3>Using your own Prompts</h3>
  Due to memory constraints on Colab, we have pre-computed a set of text embeddings for you to try. If you'd like to compute your own, you can do so as shown below:
  <pre><code class="python">

# Load the T5 text encoder
text_encoder = T5EncoderModel.from_pretrained(
    "DeepFloyd/IF-I-L-v1.0",
    subfolder="text_encoder",
    load_in_8bit=True,
    variant="8bit",
)
text_pipe = DiffusionPipeline.from_pretrained(
    "DeepFloyd/IF-I-L-v1.0",
    text_encoder=text_encoder,  # pass the previously instantiated text encoder
    unet=None
)

# Prompts to use. We'll let N = number of prompts
prompts = [
  'reading between the lions',
  'bear with me',
  'a photo of a computer vision professor',
  'a photo of a hipster barista',
  'a photo of a dog',
  'an oil painting of a snowy mountain village',
  'an oil painting of a horse',
  'a lithograph of waterfalls',
  'a lithograph of a skull',
  '',   # For CFG
]

# Get prompt embeddings using the T5 model
# each embedding is of shape [1, 77, 4096]
# 77 comes from the max sequence length that deepfloyd will take
# and 4096 comes from the embedding dimension of the text encoder
prompt_embeds = [text_pipe.encode_prompt(prompt) for prompt in prompts]
prompt_embeds, negative_prompt_embeds = zip(*prompt_embeds)
prompt_embeds_dict = dict(zip(prompts, prompt_embeds))

# Save prompt embeds
save_path = 'prompt_embeds_dict.pth'
torch.save(prompt_embeds_dict, save_path)

  </code>
    </pre>

    

<h1> Part 1: Sampling Loops</h1>
In this part of the problem set, you will write your own "sampling loops" that use the pretrained DeepFloyd denoisers. These should produce high quality images such as the ones generated above.
<p class="text">
  You will then modify these sampling loops to solve different tasks such as inpainting or producing optical illusions.
</p>

<h3>Diffusion Models Primer</h3>
<div style="text-align: center;">
    <img src="assets/ddpm_markov.png" alt="DDPM Markov Chain" style="width: 50vw; display: block; margin-left: auto; margin-right: auto" />
</div>

<p class="text">
  Starting with a clean image, $x_0$, we can iteratively add noise to an image, obtaining progressively more and more noisy versions of the image, $x_t$, until we're left with basically pure noise at timestep $t=T$. When $t=0$, we have a clean image, and for larger $t$ more noise is in the image.
</p>
<p class="text">
  A diffusion model tries to reverse this process by estimating the noise in an image. When given $x_t$ and the timestep $t$, the diffusion model predicts the noise in the image. With the predicted noise, we can either completely remove the noise from the image, to obtain an estimate of $x_0$, or we can remove just a portion of the noise, obtaining an estimate of $x_{t-1}$, with slightly less noise.
</p>
<p class="text">
  To actually sample images, we start with pure, isotropic Gaussian noise at timestep $T$, and iteratively denoise gradually until we're left with a clean image. For the DeepFloyd models, $T = 1000$.
</p>
<p class="text">
  The amount of noise added at each step is dictated by hyperparameter $\bar\alpha_t$.
</p>

<h3>1.1 Implementing the Forward Process</h3>
<p class="text">
  Write a function <code>forward(im, t)</code> to implement the forward process:
</p>
<p class="text">
  $$q(x_t | x_0) = N(x_t ; \sqrt{\bar\alpha} x_0, (1 - \bar\alpha_t)\mathbf{I})\tag{1}$$
</p>
<p class="text">
  That is, given a clean image $x_0$, return a noisy image $x_t$ at timestep $t$ by sampling from a Gaussian with mean $\sqrt{\bar\alpha} x_0$ and variance $(1 - \bar\alpha_t)$. This is how we go from a clean image to a noisy image. Note that the forward process is not just adding noise -- we also scale the image.
</p>
<p class="text">
  You will need to use the <code>alphas_cumprod</code> variable, which contains the $\bar\alpha_t$ for all $t \in [0, 999]$. 
  Remember that $t=0$ corresponds to a clean image, and larger $t$ corresponds to more noise. 
  Thus, $\bar\alpha_t$ is close to 1 for small $t$, and close to 0 for large $t$. 
  Run the forward process on the test image with $t \in [250, 500, 750]$ and display the results. You should get progressively more noisy images.
</p>
<p class="text">
  Hints:
</p>
<ul>
  <li>Sampling from the above probability distribution is the equivalent of computing $x_t = \sqrt{\bar\alpha} x_0 + \sqrt{1 - \bar\alpha} \epsilon$, for $\epsilon \sim N(0, 1)$.</li>
  <li>The <code>torch.randn_like</code> function is helpful for computing $\epsilon$.</li>
</ul>
<p class="text">
  <b>Deliverables:</b>
</p>
<ul>
  <li>Implemented <code>forward(im, t)</code> function</li>
  <li> Show a photograph $x_0$ (that you have taken yourself), as well as 3 noised versions of $x_0$ at noise levels $\sigma$ = [250, 500, 750].</li>
</ul>

<h3>1.2 Classical Denoising</h3>
<p class="text">
  Let's try to denoise these images using classical methods. 
  Again, take noisy images for timesteps [250, 500, 750], but use <b>gaussian blur filtering</b> (with kernel size 5 and sigma 2) to try to remove the noise. 
  The results should be quite bad.
</p>

<b> Deliverables: </b>
<ul>
  <li>Three noisy images at noise levels [250, 500, 750]</li>
  <li>The three noisy images processed with Gaussian blur filtering</li>
</ul>

Hint:
<ul>
  <li> The results should be quite bad.</li>
<li><code>torchvision.transforms.functional.gaussian_blur</code> is useful. 
  Here is the <a href="https://pytorch.org/vision/0.16/generated/torchvision.transforms.functional.gaussian_blur.html">documentation</a>.</li>
</ul>

<h3>1.3 One-Step Denoising</h3>

<p class="text">
  Now, we'll use a pretrained diffusion model to denoise. The actual denoiser can be found at <code>stage_1.unet</code>. This is a UNet that has been trained to be a powerful noise estimator. We can use it to estimate the noise in an image, after having run the forward process. We can then remove this noise to recover (something close to) the original image.
</p>
<p class="text">
  For t = [250, 500, 750], please:
</p>
<ul>
  <li>Run the forward process on the test image for timestep t</li>
  <li>Estimate the noise in the new noisy image, by passing it through <code>stage_1.unet</code></li>
  <li>Remove the noise from the noisy image to obtain an estimate of the original image. To do this, we can invert the equation from part 1.1: $$ x_0 = (x_t - \sqrt{1 - \bar\alpha_t}\epsilon) / \sqrt{\bar\alpha_t} $$, where $\epsilon$ is the predicted noise from <code>stage_1.unet</code>.</li>
  <li>Visualize the original image, the noisy image, and the estimate of the original image</li>
</ul>
<p class="text">
  <b>Deliverables:</b>
</p>
<ul>
  <li>Show three noisy images at timestep [250, 500, 750]</li>
  <li>For all 3 noisy images, show the diffusion model estimate of the clean image</li>
</ul>

<p class="text">
  Hints:
</p>
<ul>
  <li>You will probably have to wrangle tensors to the correct device and into the correct data types. The functions <code>.to(device)</code> and <code>.half()</code> will be useful. The denoiser is loaded as <code>half</code> precision (to save memory).</li>
  <li>The signature for the unet is <code>stage_1.unet(image, t, encoder_hidden_states=prompt_embeds, return_dict=False)</code>. You need to pass in the noisy image, the timestep, and the prompt embeddings. The <code>return_dict</code> argument just makes the output nicer.</li>
  <li>The unet will output a tensor of shape (1, 6, 64, 64). This is because DeepFloyd was trained to predict the noise as well as variance of the noise. The first 3 channels is the noise estimate, which you will use. The second 3 channels is the variance estimate which you may ignore.</li>
  <li>To save GPU memory, you should wrap all of your code in a <code>with torch.no_grad():</code> context. This tells torch not to do automatic differentiation, and saves a considerable amount of memory.</li>
</ul>

<h3>1.4 Iterative Denoising</h3>
<p class="text">
  In part 1.3, you should see that the one-step estimate of the clean image gets worse as you add more noise. This makes sense, as the problem is much harder with more noise!

  But diffusion models are designed to denoise iteratively. 
  In this part we will implement this. To do this, we can denoise taking the default steps of the model one-by-one, taking 1000 steps.
   But we can also speed things up by skipping steps. We have already created a list of timesteps, <code>timesteps</code>, which does just this. 
   There are 30 total steps, with <code>timesteps[0]</code> corresponding to the noisiest image and <code>timesteps[-1]</code> corresponding to a clean image.
</p>
<b>TODO: fix from exact timestep to pseudotimestep</b>
<p class="text">
  At every iteration, suppose we are at $ t = $ <code>timesteps[i]</code>, and want to get to $ t-1 =$ <code>timesteps[i+1]</code> (from more noisy to less noisy). 
  To get the less noisy $x_{t-1}$ we use the following formula:
</p>
$$ x_{t-1} = \frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1 - \bar\alpha_t} x_0 + \frac{\sqrt{\alpha_t}(1 - \bar\alpha_{t-1})}{1 - \bar\alpha_t} x_t + v_\sigma \tag{2}$$

<p class="text">
  The alphas and betas are hyperparameters, and are defined as below:
</p>
<ul>
  <li>$ \alpha_{\bar{t}} $ is defined by alphas_cumprod</li>
  <li>$ \alpha_t = \frac{\alpha_{\bar{t}}}{\alpha_{\bar{t-1}}} $</li>
  <li>$ \beta_t = 1 - \alpha_t $</li>
</ul>
(see equations 6 and 7 of the <a href="https://arxiv.org/pdf/2006.11239">DDPM paper</a> for more information)

<p class="text">
  The $v_\sigma$ is random noise, which in the case of DeepFloyd is also predicted. 
  The process to compute this is not very important, so we supply a function, <code>add_variance</code>, to do this for you.
</p>

<p class="text">
  Please implement the function <code>iterative_denoise(image, i_start)</code>, which takes a noisy image <code>image</code>, as well as a starting index <code>i_start</code>. 
  The function should denoise an image starting at timestep <code>timestep[i_start]</code>, applying the above formula to obtain an image at timestep <code>t_prev = timestep[i_start + 1]</code>, and repeat iteratively until we arrive at a clean image.

  Please add noise to the test image <code>im</code> to timestep <code>timestep[10]</code>. Then run the <code>iterative_denoise</code> function on the noisy image, with <code>i_start = 10</code>.
</p>

<p class="text">
  <b>Deliverables:</b>
</p>
<ul>
  <li>The noisy image every 5th loop of denoising</li>
  <li>The final predicted clean image, using iterative denoising</li>
  <li>The predicted clean image using only a single denoising step, as was done in the previous part. This should look much worse.</li>
  <li>A completed <code>iterative_denoise</code> function</li>
</ul>

Hints:
<ul>
  <li>Remember, the unet will output a tensor of shape (1, 6, 64, 64). This is because DeepFloyd was trained to predict the noise as well as variance of the noise. The first 3 channels is the noise estimate, which you will use here.
     The second 3 channels is the variance estimate which you will pass to the <code>add_variance</code> function</li>
  <li>Read the documentation for the <code>add_variance</code> function to figure out how to use it to add the $v_\sigma$ to the image.</li>
</ul>

<h3>1.5 Diffusion Model Sampling</h3>
<p class="text">
  Another thing we can do with the <code>iterative_denoise</code> function is to generate images from scratch. 
  We can do this by setting <code>i_start = 0</code> and passing in random noise. This effectively denoises pure noise. Please do this, and show 10 results of <code>"a photo of a dog"</code>.
</p>
<p class="text">
  <b>Deliverables:</b>
  <ul>
    <li>show 10 results of <code>"a photo of a dog"</code>.</li>
  </ul>
</p>
<p class="text">
  Hints:
</p>
<ul>
  <li>Use <code>torch.randn</code> to make the noise.</li>
  <li>Make sure you move the tensor to the correct device and correct data type by calling <code>.half()</code> and <code>.to(device)</code>.</li>
  <li>The quality of the images will not be spectacular, but should resemble dogs. We will fix this in the next section.</li>
</ul>

<h3>1.6 Classifier-Free Guidance (CFG)</h3>
<p class="text">
  You may have noticed that the generated images in the prior section are not very good, and some are completely non-sensical. 
  In order to greatly improve image quality (at the expense of image diversity), we can use a technicque called <a href="https://arxiv.org/abs/2207.12598">Classifier-Free Guidance</a>.
   (The name Classifier-Free Guidance comes from the fact that there was originally a technique called "classifier guidance," which used image classifiers to work. This is as opposed to CFG, which is classifier "free." 
   Now we're kinda stuck with the name for historical reasons.)
</p>
<p class="text">
  In CFG, we compute both a conditional and an unconditional noise estimate. We denote these $\epsilon_c$ and $\epsilon_u$. 
  Then, we let our new noise estimate be: $$\epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u) \tag{3}$$
  where $\gamma$ controls the strength of CFG. Notice that for $\gamma=0$, we get an unconditional noise estimate, and for $\gamma=1$ we get the conditional noise estimate. 
  The magic happens when $\gamma > 1$. In this case, we get much higher quality images. Why this happens is still up to vigorous debate. 
  For more information on CFG, you can check out <a href="https://sander.ai/2022/05/26/guidance.html">this blog post</a>.
</p>
<p class="text">
  Please implement the <code>iterative_denoise_cfg</code> function, identical to the <code>iterative_denoise</code> function but using classifier-free guidance. 
  To get an unconditional noise estimate, just pass an empty prompt embedding to the diffusion model 
  (the model was trained to predict an unconditional noise estimate when given an empty text prompt).
</p>
<p class="text">
  <b>Deliverables:</b>
  <ul>
    <li>Show 10 results of <code>"a photo of a dog"</code> with a CFG scale of $\gamma=7$.</li>
    <li>Implemented <code>iterative_denoise_cfg</code></li>
  </ul>
  Because CFG reduces diversity to increase image quality, the biases of the model should now be even more evident.
</p>
<p class="text">
  Hints:
</p>
<ul>
  <li>Because we are passing two prompts to the UNet, you will also need to duplicate the <code>image</code> argument to the UNet. Repeat the image in the batch dimension.</li>
  <li>The UNet will predict both a conditional and an unconditional variance. Just use the conditional variance with the <code>add_variance</code> function.</li>
  <li>The resulting images should be much better than those in the prior sectino.</li>
</ul>

<h3>1.7 SDEdit: Image Editing</h3>
<p class="text">
  In part 1.4, we take a real image, add noise to it, and then denoise. This effectively allows us to "remix" or "edit" the original image. The more noise we add, the larger the edit will be. 
  This works because in order to denoise an image, the diffusion model must to some extent "hallucinate" new things -- the model has to be "creative." Another way to think about it is that the denoising process "projects" a noisy image back to the "manifold" or "set" of real images. (For more info, please see the <a href="https://sde-image-editing.github.io/">SDEdit paper</a>).
</p>
<p class="text">
  With this in mind, please run the forward process to get a noisy test image, and then run the <code>iterative_denoise_cfg</code> function using a starting index of [1, 3, 5, 7, 10, 20] steps and show the results, labeled with the starting index.
  You should see a series of "edits" to the original image, gradually matching the original image closer and closer.
</p>
<b> Deliverables: </b>
<ul>
  <li>Show the results of the SDEdit process for $t = [1, 5, 10, 15, 20, 25]$ for the prompt <code>"a photo of a hipster barista"</code></li>
</ul>
<p class="text">
  Hints:
  <ul>
    <li>The images should gradually look more like the original image.</li>
  </ul>
</p>

<h3>1.8 Inpainting</h3>
<p class="text">
  We can also use diffusion models to inpaint. That is, given an image $x_{orig}$, and a binary mask $\bf m$, we can create a new image that has the same content where $\bf m$ is 0, but new content wherever $\bf m$ is 1.
</p>
<p class="text">
  To do this, we can run the diffusion denoising loop. But at every step, after obtaining $x_t$, we "project" $x_t$ such that areas outside of the mask aligns with the original image $x_{orig}$. To do this, we can use the following update:
</p>
<p class="text">
  $$ x_t \leftarrow \textbf{m} x_t + (1 - \textbf{m}) \text{forward}(x_{orig}, t) $$
</p>
<p class="text">
  Essentially, we leave everything inside the edit mask alone, but we replace everything outside the edit mask with our original image, with the correct amount of noise added for timestep $t$. For more information about this method, please see this <a href="https://arxiv.org/abs/2201.09865">paper</a>.
</p>
<p class="text">
  Please implement this below, and edit the picture to add a hat. Please display the image.
</p>
<p class="text">
  <b>Deliverables:</b>
  
  <ul>
    <li>Edit the picture to add a hat.</li>
    <li>Display the image.</li>
  </ul>
  Run the following cell to retrieve the correct prompt embedding, and an edit mask.
</p>
<p class="text">
  Hints:
</p>
<ul>
  <li>Reuse the <code>forward</code> function you implemented earlier to implement inpainting</li>
  <li>Because we are using the diffusion model for tasks it was not trained for, you may have to run the sampling process a few times before you get a nice result.</li>
  <li>You can copy and paste your iterative_denoise_cfg function. To get inpainting to work should only require (roughly) 1-2 additional lines and a few small changes.</li>
</ul>

<h3>1.9 Visual Anagrams</h3>
<p class="text">
  We can also create optical illusions with diffusion models.
  In this part, we will create an image that looks like <code>"an oil painting of a snowy mountain village"</code>,
  but when flipped upside down will reveal <code>"an oil painting of a horse"</code>.
</p>
<p class="text">
  To do this, we will denoise an image normally with the prompt <code>"an oil painting of a horse"</code>, to obtain noise estimate $\epsilon_1$.
  But at the same time, we will take our noisy image, flip it upside down, 
  and denoise with the prompt <code>"an oil painting of a snowy mountain village"</code>, to get noise estimate $\epsilon_2$.
  We can flip $\epsilon_2$ again, to make it right-side up, and average the two noise estimates. And run DDPM with the averaged noise estimate.
</p>
<p class="text">  
  The full algorithm will be:
</p>
<p class="text">
  $$ \epsilon_1 = u(x_t, t, p_1) $$
</p>
<p class="text">
  $$ \epsilon_2 = f(u(f(x_t), t, p_2)) $$
</p>
<p class="text">
  $$ \epsilon = (\epsilon_1 + \epsilon_2) / 2 $$
</p>
<p class="text">
  where $u$ is the diffusion model unet, $f$ is a function that flips the image, and $p_1$ and $p_2$ are two different text prompt embeddings. 
  And our final noise estimate is $\epsilon$. Please show an example of an illusion (you may have to run multiple times to get a really good result for the same reasons as above).
</p>

<b>Deliverables:</b>
<ul>
  <li>Correctly implemented <code>visual_anagrams</code> function</li>
  <li>An illusions that changes appearance when you flip it upside down</li>
</ul>

<h3>2.0 Hybrid Images</h3>
<p class="text">
  Hybrid images are images that look different depending on distance. 
  These images work because from a distance, we can only see the low frequencies of an image, while up close we can see both high and low frequencies.
</p>
<p class="text">
  In order to create hybrid images with a diffusion model we can use a similar technique as above. 
  We will create a composite noise estimate $\epsilon$, by estimating the noise with two different text prompts, 
  and then combining low frequencies from one noise estimate with high frequencies of the other. 
  The algorithm is:
</p>
<p class="text">
  $$ \epsilon_1 = u(x_t, t, p_1) $$
</p>
<p class="text">
  $$ \epsilon_2 = u(x_t, t, p_2) $$
</p>
<p class="text">
  $$ \epsilon = f_{low}(\epsilon_1) + f_{high}(\epsilon_2) $$
</p>
<p class="text">
  where $u$ is the diffusion model unet, $f_{low}$ is a low pass function, $f_{high}$ is a high pass function, 
  and $p_1$ and $p_2$ are two different text prompt embeddings. Our final noise estimate is $\epsilon$. 
  Please show an example of a hybrid image using this technique (you may have to run multiple times to get a really good result for the same reasons as above).
</p>

<b>Deliverables:</b>
<ul>
  <li>Correctly implemented <code>make_hybrids</code> function</li>
  <li>An image that looks like a skull from far away but a waterfall from close up</li>
</ul>

Hints:
<ul>
  <li>use torchvision.transforms.functional.gaussian_blur. The documentation can be found <a href="https://pytorch.org/vision/0.16/generated/torchvision.transforms.functional.gaussian_blur.html">here</a>.</li>
</ul>

<h1> Part 2: Bells & Whistles </h1>

<ul>
  <li>Create a course logo! The best logo will (possibly) be used to design a class t-shirt, and the winner will get one for free!</li>
</ul>


<!--     

    <h1>Part 0: Sampling from the model</h1>
    <h3>Gaining Access to DeepFloyd</h3>
  <p class="text">
    We are going to use the <a href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if">DeepFloyd IF</a> diffusion model. DeepFloyd is a two stage model trained by Stability AI. The first stage produces images of size $64 \times 64$ and the second stage takes the outputs of the first stage and generates images of size $256 \times 256$. Before using DeepFloyd, you must accept its usage conditions. To do so:
  </p>
  <ol>
    <li>Make a <a href="https://huggingface.co/join">Hugging Face account</a> and log in.</li>
    <li>Accept the license on the model card of <a href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0">DeepFloyd/IF-I-XL-v1.0</a>. Accepting the license on the stage I model card will auto accept for the other IF models.</li>
    <li>Log in locally by entering your <a href="https://huggingface.co/docs/hub/security-tokens#what-are-user-access-tokens">Hugging Face Hub access token</a> below. You should be able to find and create tokens <a href="https://huggingface.co/settings/tokens">here</a>.</li>
  </ol>

  <h3>Loading DeepFloyd Models</h3>
  <p class="text">
    We will need to download and create the two DeepFloyd stages. These models are quite large, so this block may take a while the first time you run it (~5 min) so plan ahead! After the first time, Colab should cache the files for a while (i.e. you can hit Runtime -> Restart Session if you hit a weird error and the models will still be downloaded).
  </p>

  Once you have an access token with the necessary permissions, you can load the models as shown here: 
  <pre><code class="python">
from huggingface_hub import login

token = 'YOUR ACCESS TOKEN'
login(token=token)

# Load DeepFloyd IF stage I
stage_1 = DiffusionPipeline.from_pretrained(
    "DeepFloyd/IF-I-L-v1.0",
    text_encoder=None,
    variant="fp16",
    torch_dtype=torch.float16,
)
stage_1.to(device)

# Load DeepFloyd IF stage II
stage_2 = DiffusionPipeline.from_pretrained(
                "DeepFloyd/IF-II-L-v1.0",
                text_encoder=None,
                variant="fp16",
                torch_dtype=torch.float16,
              )
stage_2.to(device)


  </code>
  </pre>

<h3>Downloading Text Embeddings</h3>
In order to use the diffusion model, we will need to condition on text embeddings. 
However, because the text encoder is very large, and barely fits on a free tier Colab GPU, 
we have precomputed a couple of text embeddings for you to try. 
This should hopefully save some headaches with respect to OOM errors.
They can be found <a href="./parta.html">here</a>.




  </body> -->
  <grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>
</html>
