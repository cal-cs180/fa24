
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>CS180: Intro to Computer Vision and Computational Photography</title>
    <link rel="StyleSheet" href="../style.css" type="text/css" media="all" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <link rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css" />
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
    <script type="module" src="your-compiled-script.js"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true
          }
        };
      </script>

    <script>
      hljs.initHighlightingOnLoad();
    </script>
    <style>
        code {
            background-color: #f4f4f4;
            padding: 5px;
            border-radius: 5px;
        }
        .image-container {
  display: flex;
  justify-content: center;
  align-items: flex-start;
  gap: 20px;
}

.image-container > div {
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 30%;
  max-width: 200px;
}

.image-container img {
  width: 100%;
  height: auto;
}

.image-container p {
  text-align: center;
  margin-top: 10px;
}
    /* Two image containers */
    .column {
      float: left;
      width: 45%;
      padding: 5px;
    }

    /* Clear floats after image containers */
    .row::after {
      content: "";
      clear: both;
      display: table;
    }
    code {
          background-color: #f4f4f4;
          padding: 2.5px;
          border-radius: 5px;
      }
        .image-container {
  display: flex;
  justify-content: center;
  align-items: flex-start;
  gap: 20px;
}

        .image-container {
  display: flex;
  justify-content: center;
  align-items: flex-start;
  gap: 20px;
}

.image-container > div {
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 40%;
  max-width: 200px;
  position: relative;
  padding: 20px;
  overflow: visible;
}

.image-container img {
  width: 100%;
  height: auto;
  transform-origin: center center;
}

.image-container p {
  text-align: center;
  margin-top: 10px;
}
      /* Two image containers */
      .column {
        float: left;
        width: 45%;
        padding: 5px;
      }

      /* Clear floats after image containers */
      .row::after {
        content: "";
        clear: both;
        display: table;
      }

      @keyframes rotate180 {
        from {
          transform: rotate(0deg);
        }
        to {
          transform: rotate(180deg);
        }
      }
      
      .rotating-image {
        transition: transform 1.5s;
        transform: rotate(0deg);
      }
      
      .rotating-image:hover {
        transform: rotate(180deg);
      }

      .zoom-animation {
        transition: transform 1s ease-in-out;
        transform: scale(1);
      }

      .zoom-animation:hover,
      .zoom-animation.active {
        transform: scale(0.25);
      }

      .rotating-image {
        transition: transform 1.5s;
        transform: rotate(0deg);
      }

      .rotating-image:hover,
      .rotating-image.active {
        transform: rotate(180deg);
      }

      .caption-container {
        position: relative;
        height: auto;
        min-height: 2em;
        text-align: center;
        width: 100%;
        padding: 5px 0;
      }

      .caption-default, .caption-transform {
        position: absolute;
        width: 100%;
        transition: opacity 1.5s;
        white-space: normal;
        left: 0;
      }

      .caption-transform {
        opacity: 0;
      }

      .rotating-image:hover + .caption-container .caption-default,
      .active + .caption-container .caption-default {
        opacity: 0;
      }

      .rotating-image:hover + .caption-container .caption-transform,
      .active + .caption-container .caption-transform {
        opacity: 1;
      }

      .zoom-animation:hover + .caption-container .caption-default {
        opacity: 0;
      }

      .zoom-animation:hover + .caption-container .caption-transform {
        opacity: 1;
      }

      .image-container > div:hover .zoom-animation {
        transform: scale(0.25);
      }

      .image-container > div:hover .caption-default {
        opacity: 0;
      }

      .image-container > div:hover .caption-transform {
        opacity: 1;
      }

      .caption-container .caption-default {
        opacity: 1;
        transition: opacity 1.5s;
      }

      .caption-container .caption-transform {
        opacity: 0;
        transition: opacity 1.5s;
      }

      .active + .caption-container .caption-default {
        opacity: 0;
      }

      .active + .caption-container .caption-transform {
        opacity: 1;
      }

      .dissolve-container {
        position: relative;
        width: 100%;
        height: 0;
        padding-bottom: 100%; /* Creates a square aspect ratio */
        margin-bottom: 5px; /* Reduced from 10px to match other captions */
      }

      .dissolve-image {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        transition: opacity 1s ease-in-out;
      }

      .dissolve-image.original {
        opacity: 1;
      }

      .dissolve-image.edited {
        opacity: 0;
      }

      .dissolve-image.original.active {
        opacity: 0;
      }

      .dissolve-image.edited.active {
        opacity: 1;
      }

      /* Ensure consistent caption styling */
      .image-container > div p {
        text-align: center;
        margin-top: 5px;  /* Reduced from 10px to align with other captions */
        margin-bottom: 0;
      }

      /* Hover state */
      .dissolve-container:hover .dissolve-image.original {
        opacity: 0;
      }

      .dissolve-container:hover .dissolve-image.edited {
        opacity: 1;
      }

      /* Add styling for code comments */
      code .hljs-comment {
        color: #666666;  /* A dark grey color */
      }

      h1 {
      font-size: x-large;
    }

    h1 a {
      font-size: medium;
    }

    h1 img {
      float: left;
      padding-right: 1em;
    }
    </style>

  </head>

  <body data-new-gr-c-s-check-loaded="14.1027.0" data-gr-ext-installed>
    <h1>
      <a href="http://www.berkeley.edu/" style="text-decoration: none;">
        <img
          src="assets/ucbseal.png"
          alt="berkeley logo"
          height="75"
          width="75"/>
      </a>
      Programming Project #5 (<code>proj5</code>)<br>
      <a href="https://cal-cs180.github.io/fa24/">CS180: Intro to Computer Vision and Computational
        Photography</a>
      <br>
      <a href="http://www.berkeley.edu/"
                  >University of California, Berkeley</a
                >
      
    </h1>
    <div>

      <video id="video5" width="640" height="320" muted
        style="display: block; margin-left: auto; margin-right: auto;"
        onmouseover="handleMouseOver(this)"
        onmouseout="handleMouseOut(this)">
        <source type="video/mp4" src="assets/mnist.mp4" />
      </video>
    </div>
    <h1 style="text-align: center">Part B: Diffusion Models from Scratch!</h1>
    <h3 style="text-align: center;">The first part of a <a
        href="./index.html">larger project</a>.</h3>
    <h2 style="text-align: center">
      <b style="color: red;">Due: 11/19/24 11:59pm</b>
    </h2>
    <h4 style="text-align: center">
      <b>We recommend using GPUs from <a
          href="https://colab.research.google.com/">Colab</a> to finish this
        project!</b>
    </h4>

    <h2>Overview</h2>
    In part B you will train your own diffusion model on MNIST. Starter code can
    be found in the <a
      href="https://colab.research.google.com/drive/1CnyXxgexPj3zPIFQB9ju7SpfqtjFdjId?usp=sharing">provided
      notebook</a>.
    <br>
    <br>
    <p style="color: #CC0000; display: inline;"><b>START EARLY!</b></p><span style="margin-left: 5px;"> This project, in many ways, will be the most difficult project this semester.</span>

    <p>Note: this is an updated, clearer version of the part B instructions. For the old version, please see <a href="partb_old.html">here</a>.</p>

    <h1>Part 1: Training a Single-Step Denoising UNet</h1>
    <p class="text">
      Let's warmup by building a simple one-step denoiser. Given a noisy image
      $z$, we
      aim to train a denoiser $D_\theta$ such that it maps $z$ to a clean
      image $x$. To do so, we can optimize over an L2 loss:
      $$L = \mathbb{E}_{z,x} \|D_{\theta}(z) - x\|^2 \tag{B.1}$$
    </p>
    <h2> 1.1 Implementing the UNet</h2>
    In this project, we implement the denoiser as a <a
      href="https://arxiv.org/abs/1505.04597"> UNet</a>. It consists of a
    few downsampling and upsampling blocks with skip connections.
    <br>
    <br>
    <div style="text-align: center;">
      <img src="assets/unconditional_arch.png" alt="UNet Architecture" height="500"
        style="display: block; margin-left: auto; margin-right: auto" />
      <p class="text">Figure 1: Unconditional UNet</p>
    </div>

    <p>The diagram above uses a number of standard tensor operations defined as
      follows:</p>
    <div style="text-align: center;">
      <img src="assets/atomic_ops_new.png" alt="UNet Operations" height="400"
        style="display: block; margin-left: auto; margin-right: auto" />
      <p class="text">Figure 2: Standard UNet Operations</p>
    </div>

    <br\>

      where:
      <ul>
        <li><b><tt>Conv2d(kernel_size, stride, padding)</tt></b> is
          <code>nn.Conv2d()</code></li>
        <li><b><tt>BN</tt></b> is <code>nn.BatchNorm2d()</code></li>
        <li><b><tt>GELU</tt></b> is <code>nn.GELU()</code></li>
        <li><b><tt>ConvTranspose2d(kernel_size, stride, padding)</tt></b> is
          <code>nn.ConvTranspose2d()</code></li>
        <li><b><tt>AvgPool(kernel_size)</tt></b> is
          <code>nn.AvgPool2d()</code></li>
          <li><code>D</code> is the number of hidden channels and is a
            hyperparameter that we will set ourselves.</li>
      </ul>

      At a high level, the blocks do the following:
      <ul>
        <li><b><tt>(1) Conv</tt></b> is a convolutional layer that doesn't
          change the image resolution, only the channel dimension.</li>
        <li><b><tt>(2) DownConv</tt></b> is a convolutional layer that
          downsamples the tensor by 2.</li>
        <li><b><tt>(3) UpConv</tt></b> is a convolutional layer that upsamples
          the tensor by 2.</li>
        <li><b><tt>(4) Flatten</tt></b> is an average pooling layer that
          flattens a 7x7 tensor into a 1x1 tensor. 7 is the resulting height and
          width after the downsampling operations.</li>
        <li><b><tt>(5) Unflatten</tt></b> is a convolutional layer that
          unflattens/upsamples a 1x1 tensor into a 7x7 tensor.</li>
        <li><b><tt>(6) Concat</tt></b> is a channel-wise concatenation between
          tensors with the same 2D shape. This is simply
          <code>torch.cat()</code>.</li>
      </ul>

      <p class="text">
        We define composed operations using our simple operations in order to
        make our network deeper. This doesn't change the tensor's height, width,
        or number of channels, but simply adds more learnable parameters.
        <ul>
          <li><b><tt>(7) ConvBlock</tt></b>, is similar to <b><tt>Conv</tt></b>
            but includes an additional <b><tt>Conv</tt></b>. Note that it has
            the same input and output shape as <b><tt> (1) Conv</tt></b>.</li>
          <li><b><tt>(8) DownBlock</tt></b>, is similar to
            <b><tt>DownConv</tt></b> but includes an additional
            <b><tt>ConvBlock</tt></b>. Note that it has the same input and
            output shape as <b><tt> (2) DownConv</tt></b>.</li>
          <li><b><tt>(9) UpBlock</tt></b>, is similar to <b><tt>UpConv</tt></b>
            but includes an additional <b><tt>ConvBlock</tt></b>. Note that it
            has the same input and output shape as <b><tt> (3)
                UpConv</tt></b>.</li>
        </ul>
      </p>

      <h2> 1.2 Using the UNet to Train a Denoiser</h2>
      Recall from equation 1 that we aim to solve the following denoising
      problem:

      Given a noisy image $z$, we
      aim to train a denoiser $D_\theta$ such that it maps $z$ to a clean
      image $x$. To do so, we can optimize over an L2 loss
      $$
      L = \mathbb{E}_{z,x} \|D_{\theta}(z) - x\|^2.
      $$

      To train our denoiser, we need to generate training data pairs of ($z$,
      $x$), where each $x$ is a clean MNIST digit. For each training batch, we
      can generate $z$ from $x$ using the the following noising process:
      $$
      z = x + \sigma \epsilon,\quad \text{where }\epsilon \sim N(0, I). \tag{B.2}
      $$

      Visualize the different noising processes over $\sigma = [0.0, 0.2, 0.4,
      0.5, 0.6, 0.8, 1.0]$, assuming normalized $x \in [0, 1]$.

      It should be similar to the following plot:
      <div style="text-align: center;">
        <img src="assets/varying_sigma.png" alt="Varying Sigmas" height="600"
          style="display: block; margin-left: auto; margin-right: auto" />
        <p class="text">Figure 3: Varying levels of noise on MNIST digits</p>
      </div>
      <h2>1.2.1 Training</h2>
      <p class="text">
        Now, we will train the model to perform denoising.
      </p>
      <ul>
        <li><b>Objective:</b> Train a denoiser to denoise noisy image $z$ with
          $\sigma = 0.5$ applied to a clean image $x$.</li>

        <li><b>Dataset and dataloader:</b> Use the MNIST dataset via
          <code>torchvision.datasets.MNIST</code> with flags to access training
          and test sets. Train only on the training set. Shuffle the dataset
          before creating the dataloader. Recommended batch size: 256. We'll
          train over our dataset for 5 epochs.

          <ul>
            <li>You should only noise the image batches when fetched from the
              dataloader so that in every epoch the network will see new noised
              images, improving generalization.</li>
          </ul>

        </li>

        <li><b>Model:</b> Use the UNet architecture defined in section 1.1 with
          recommended hidden dimension <code>D = 128</code>.</li>

        <li><b>Optimizer:</b> Use Adam optimizer with learning rate of
          1e-4.</li>
      </ul>
      <div style="text-align: center;">
        <img src="assets/training_losses_uncond.png" alt="Training Loss Curve"
          height="400"
          style="display: block; margin-left: auto; margin-right: auto" />
        <p class="text">Figure 4: Training Loss Curve</p>
      </div>

      <p class="text"></p>
      You should visualize denoised results on the test set at the end of
      training. Display sample results after the 1st and 5th epoch.
    </p>
    <p class="text">
      They should look something like these:
    </p>
    <div style="text-align: center;">
      <img src="assets/unet_sample_epoch0.png" alt="After the first epoch"
        height="400"
        style="display: block; margin-left: auto; margin-right: auto" />
      <p class="text">Figure 5: Results on digits from the test set after 1
        epoch of training</p>
    </div>
    <div style="text-align: center;">
      <img src="assets/unet_sample_epoch5.png" alt="After the 5-th epoch"
        height="400"
        style="display: block; margin-left: auto; margin-right: auto" />
      <p class="text">Figure 6: Results on digits from the test set after 5
        epochs of training</p>
    </div>

    <h2>1.2.2 Out-of-Distribution Testing</h2>

    <p class="text">
      Our denoiser was trained on MNIST digits noised with $\sigma = 0.5$. Let's
      see how the denoiser performs on different $\sigma$'s that it wasn't
      trained for.
    </p>
    <p class="text">
      Visualize the denoiser results on test set digits with varying levels of
      noise $\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]$.
    </p>
    <div style="text-align: center;">
      <img src="assets/out_of_distribution2.png" alt="Varying Sigmas"
        style="max-width: 90%; height: auto; display: block; margin-left: auto; margin-right: auto" />
      <p class="text">Figure 7: Results on digits from the test set with varying
        noise levels.</p>
    </div>

    <h2> Deliverables </h3>
    <ul>
      <li>A visualization of the noising process using $\sigma = [0.0,
        0.2, 0.4, 0.5, 0.6, 0.8, 1.0]$. (figure 3)</li>
      <li>A training loss curve plot every few iterations during the whole
        training process (figure 4).</li>
      <li>Sample results on the test set after the first and the 5-th epoch
        (staff solution takes ~7 minutes for 5 epochs on a Colab T4 GPU).
        (figure 5, 6)</li>
      <li>Sample results on the test set with out-of-distribution noise levels
        after the model is trained. Keep the same image and
        vary $\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]$. (figure 7)</li>
    </ul>

    <b>Hint</b>
    <ul>
      <li>Since training can take a while, <b>we strongly recommend that you
          checkpoint your model</b> every epoch onto your personal Google
        Drive.
        This is because Colab notebooks aren't persistent such that if you are
        idle for a while, you will lose connection and your training progress.
        This consists of: <ul>
          <li>Google Drive mounting.</li>
          <li>Epoch-wise model & optimizer checkpointing.</li>
          <li>Model & optimizer resuming from checkpoints.</li>
        </ul>
      </li>
    </ul>

    <h1>Part 2: Training a Diffusion Model</h1>
    Now, we are ready for diffusion, where we will train a UNet model that can iteratively denoise an image.
    We will implement <a href="https://arxiv.org/abs/2006.11239">DDPM</a> in
    this part.

    <p class="text">
      Let's revisit the problem we solved in equation B.1:
    </p>

    <p class="text" style="text-align: center;">
      $$L = \mathbb{E}_{z,x} \|D_{\theta}(z) - x\|^2.$$
    </p>

    <p class="text"></p>
    We will first introduce one small difference: we can change our UNet to predict
    the added noise $\epsilon$ instead of the clean image $x$ (like in part A
    of the project).
    Mathematically, these are equivalent since $x = z - \sigma \epsilon$ (equation B.2).
    Therefore, we can turn equation B.1 into the following:
  </p>

  <p class="text" style="text-align: center;">
    $$L = \mathbb{E}_{\epsilon,z} \|\epsilon_{\theta}(z) - \epsilon\|^2
    \tag{B.3}$$
  </p>

  <p>where $\epsilon_\theta$ is a UNet trained to predict noise. </p>
  <p>
    For diffusion, we eventually want to sample a pure noise image $\epsilon
    \sim N(0, I)$ and generate a realistic image $x$ from the noise.
    However, we saw in part A that one-step denoising does not
    yield good results. Instead, we need to <em>iteratively</em> denoise the
    image for better results.
  </p>

  <p>
    Recall in part A that we used equation A.2 to generate noisy images $x_t$
    from $x_0$ for some timestep $t$ for $t \in \{0, 1, \cdots, T\}$:
    $$ x_t = \sqrt{\bar\alpha_t} x_0 + \sqrt{1 - \bar\alpha_t} \epsilon
    \quad \text{where}~ \epsilon \sim N(0, 1).$$

    Intuitively, when $t = 0$ we want $x_t$ to be the clean image $x_0$, when $t = T$ we want $x_t$ to be pure noise $\epsilon$, and for $t \in \{1, \cdots, T-1\}$, $x_t$ should be some linear combination of the two.

    The precise derivation of $\bar\alpha$ is beyond the scope of this project
    (see <a href="https://arxiv.org/abs/2006.11239">DDPM paper</a> for more
    details). Here, we provide you with the DDPM recipe to build a list $\bar\alpha$ for $t \in \{0, 1, \cdots, T\}$ utilizing lists $\alpha$ and $\beta$:

    <ul>
      <li>Create a list $\beta$ of length $T$ such that $\beta_0 = 0.0001$ and $\beta_T = 0.02$ and all other elements $\beta_t$ for $t \in \{1, \cdots, T-1\}$ are evenly spaced between the two.</li>
      <li>$\alpha_t = 1 - \beta_t$</li>
      <li>$\bar\alpha_t = \prod_{s=1}^t \alpha_s$ is a cumulative product of $\alpha_s$ for $s \in \{1, \cdots, t\}$.</li>
    </ul>
    Because we are working with simple MNIST digits, we can afford to have a smaller $T$ of 300 instead of the 1000 used in part A. Observe how $\bar\alpha_t$ is close to 1 for small $t$ and close to 0 for $T$. $\beta$ is known as the variance schedule; it controls the amount of noise added at each timestep.

  </p>

  </p>

  <p class="text"">
     Now, to denoise image $x_t$, we could simply apply our UNet $\epsilon_\theta$ on $x_t$ and get the noise $\epsilon$. However, this won't work very well because the UNet is expecting the noisy image to have a noise variance $\sigma = 0.5$ for best results, but the variance of $x_t$ varies with $t$. 
     One could train $T$ separate UNets, but it is much
     easier to simply condition a single UNet with timestep $t$, giving us our final objective:

    $$L = \mathbb{E}_{\epsilon,x_0,t} \|\epsilon_{\theta}(x_t, t) -
    \epsilon\|^2. \tag{B.4}$$
  </p>

  <h2> 2.1 Adding Time Conditioning to UNet</h2>
  We need a way to inject scalar $t$ into our UNet model to condition it. There are many ways to do this. Here is what we suggest:

  <div style="text-align: center;"></div>
    <div style="text-align: center;">
      <img src="assets/conditional_arch.png" alt="UNet Highlighted" height="500" />
      <p class="text">Figure 8: Conditioned UNet</p>
    </div>
  </div>

  <p class="text">This uses a new operator called
    <b><tt>FCBlock</tt></b> (fully-connected block) which we use to inject the conditioning signal into the UNet:</p>
  <div style="text-align: center;">
    <img src="assets/fc_long.png" alt="FCBlock" height="200"
      style="display: block; margin-left: auto; margin-right: auto" />
    <p class="text">Figure 9: FCBlock for conditioning</p>
  </div>
  Here <b><tt>Linear(F_in, F_out)</tt></b> is a linear layer with
  <b><tt>F_in</tt></b> input features and <b><tt>F_out</tt></b> output
  features. You can implement it using <code>nn.Linear</code>.

  <p class="text">Since our conditioning signal $t$ is a scalar, <b><tt>F_in</tt></b> should be of size 1. We also recommend that you normalize $t$ to be in the range [0, 1] before embedding it, i.e. pass in $\frac{t}{T}$.
    

  <p class="text">
    You can embed $t$ by following this pseudo code:
  </p>
  <div style="width: 50%;">
    <pre><code class="language-python">
fc1_t = FCBlock(...)
fc2_t = FCBlock(...)

# the t passed in here should be normalized to be in the range [0, 1]
t1 = fc1_t(t)
t2 = fc2_t(t)

# Follow diagram to get unflatten.
# Replace the original unflatten with modulated unflatten.
unflatten = unflatten + t1
# Follow diagram to get up1.
...
# Replace the original up1 with modulated up1.
up1 = up1 + t2
# Follow diagram to get the output.
...
    </code></pre>
  </div>
  <h2> 2.2 Training the UNet</h2>
  Training our time-conditioned UNet $\epsilon_\theta(x_t, t)$ is now pretty easy. Basically, we pick a random image from the training set, a random $t$, and train the denoiser to predict the noise in $x_t$ We repeat this for different images and different $t$ values until the model converges and we are happy.

  <br>
  <br>

  <div style="text-align: center;">
    <img src="assets/algo1_t_only.png" alt="Algorithm Diagram"
      style="width: 30%; display: block; margin-left: auto; margin-right: auto" />
    <p class="text">Algorithm B.1. Training time-conditioned UNet</p>
  </div>

  <ul>
    <li><b>Objective:</b> Train a time-conditioned UNet $\epsilon_\theta(x_t, t)$ to predict the noise in $x_t$ given a noisy image $x_t$ and a timestep $t$.</li>

    <li><b>Dataset and dataloader:</b> Use the MNIST dataset via
      <code>torchvision.datasets.MNIST</code> with flags to access training
      and test sets. Train only on the training set. Shuffle the dataset
      before creating the dataloader. Recommended batch size: 128. We'll
      train over our dataset for 20 epochs since this task is more difficult than part A.
      <ul>
        <li>As shown in algorithm B.1, You should only noise the image batches when fetched from the
          dataloader.</li>
      </ul>

    </li>

    <li><b>Model:</b> Use the time-conditioned UNet architecture defined in section 2.1 with
      recommended hidden dimension <code>D = 64</code>. Follow the diagram and pseudocode for how to inject the conditioning signal $t$ into the UNet. Remember to normalize $t$ before embedding it.</li>

    <li><b>Optimizer:</b> Use Adam optimizer with an initial learning rate of
      1e-3. We will be using an exponential learning rate decay scheduler with a gamma of $0.1^{(1.0 / \text{num_epochs})}$. This can be implemented using <code>scheduler = torch.optim.lr_scheduler.ExponentialLR(...)</code>. You should call <code>scheduler.step()</code> after every epoch.</li>
  </ul>
<div style="text-align: center;">
  <img src="assets/t_cond_training.png" alt="Loss Curve" height="300" />
  <p class="text">Figure 10: Time-Conditioned UNet training loss curve</p>
</div>


<h2> 2.3 Sampling from the UNet</h2>
The sampling process is very similar to part A, except we don't need to predict the variance like in the DeepFloyd model. Instead, we can use our list $\beta$. 
<br>
<br>
<div style="text-align: center;">
  <img src="assets/algo2_t_only.png" alt="Algorithm Diagram"
  style="width: 30%; display: block; margin-left: auto; margin-right: auto" />
  <p class="text">Algorithm B.2. Sampling from time-conditioned UNet</p>
</div>

<div class="image-container"
        style="justify-content: center; max-width: 1200px; margin: 0 auto;">
        <div style="width: 100%; max-width: 600px;">
          
          <video id="video1" width="100%" muted loop playbackRate="0.75"
            style="display: block; margin-left: 0;">
            <source type="video/mp4" src="assets/t_only_e1.mp4" />
          </video>
          <p style="text-align: left;">Epoch 1</p>
        </div>
        <div style="width: 100%; max-width: 600px;">
          
          <video id="video2" width="100%" muted loop playbackRate="0.75"
            style="display: block; margin-left: 0;">
            <source type="video/mp4" src="assets/t_only_e5.mp4" />
          </video>
          <p style="text-align: left;">Epoch 5</p>
        </div>
      </div>

      <!-- Second row with 2 videos -->
      <div class="image-container"
        style="justify-content: center; max-width: 1200px; margin: 20px auto 0;">
        <div style="width: 100%; max-width: 600px;">
          
          <video id="video3" width="100%" muted loop playbackRate="0.75"
            style="display: block; margin-left: 0;">
            <source type="video/mp4" src="assets/t_only_e10.mp4" />
          </video>
          <p style="text-align: left;">Epoch 10</p>
        </div>
        <div style="width: 100%; max-width: 600px;">
          
          <video id="video4" width="100%" muted loop playbackRate="0.75"
            style="display: block; margin-left: 0;">
            <source type="video/mp4" src="assets/t_only_e15.mp4" />
          </video>
          <p style="text-align: left;">Epoch 15</p>
        </div>
      </div>

      <!-- Third row with 1 centered video -->
      <div class="image-container"
        style="justify-content: center; max-width: 1200px; margin: 20px auto 0;">
        <div style="width: 100%; max-width: 600px;">
          
          <video id="video5" width="100%" muted loop playbackRate="0.75"
            style="display: block; margin-left: 0;">
            <source type="video/mp4" src="assets/t_only_e20.mp4" />
          </video>
          <p style="text-align: left;">Epoch 20</p>
        </div>
      </div>

      <h2> Deliverables</h3>
      <ul>
        <li>A training loss curve plot for the time-conditioned UNet over the whole
          training process (figure 10).</li>
        <li>Sampling results for the time-conditioned UNet for 5 and 20 epochs. 
<ul>
  <li>Note: providing a gif is optional and can be done as a bells and whistles below.</li>
</ul>

        </li>
      </ul>


      <h2> 2.4 Adding Class-Conditioning to UNet</h2>
      To make the results better and give us more control for image generation, we can also optionally condition our UNet on the class of the digit 0-9. This will require adding 2 more <b><tt>FCBlock</tt></b>s to our UNet but, we suggest that for class-conditioning vector $c$, you make it a one-hot vector instead of a single scalar. 

      Because we still want our UNet to work without it being conditioned on the class, we implement dropout where 10% of the time ($p_{\text{uncond}}= 0.1$) we drop the class conditioning vector $c$ by setting it to 0.
Here is one way to condition our UNet $\epsilon_\theta(x_t, t, c)$ on both time $t$ and class $c$:

<!-- <p class="text"></p>
        An easy way to implement the dropout is to have the model take in a 
        <b>batch-wise</b> mask vector which consists of 1s and 0s.
        The values indicate whether or not to drop the condition $c$: drop when mask
        is 0, not drop when mask is 1. $p_{\text{uncond}}= 0.1$ means we set a value to 0 with 10% probability. 
      </p> -->

      <div style="width: 40%;">
        <pre><code class="language-python">
fc1_t = FCBlock(...)
fc1_c = FCBlock(...)
fc2_t = FCBlock(...)
fc2_c = FCBlock(...)

t1 = fc1_t(t)
c1 = fc1_c(c)
t2 = fc2_t(t)
c2 = fc2_c(c)

# Follow diagram to get unflatten.
# Replace the original unflatten with modulated unflatten.
unflatten = c1 * unflatten + t1
# Follow diagram to get up1.
...
# Replace the original up1 with modulated up1.
up1 = c2 * up1 + t1
# Follow diagram to get the output.
...



        </code></pre>
        Training for this section will be the same as time-only, with the only difference being the conditioning vector $c$ and doing unconditional generation periodically.
      </div>

      <div style="text-align: center;">
        <img src="assets/algo3_c.png" alt="Algorithm Diagram"
        style="width: 30%; display: block; margin-left: auto; margin-right: auto" />
        <p class="text">Algorithm B.3. Training class-conditioned UNet</p>
      </div>
      

      <div style="text-align: center;"></div>
        <div style="text-align: center;">
          
          <img src="assets/correct_c_losses.png" alt="Training Loss Curve"
            style="width: 500px; height: auto; display: block; margin-left: auto; margin-right: auto" />
            <p class="text">Figure 11: Class-conditioned UNet training loss curve</p>
        </div>
      </div>

      <h2> 2.5 Sampling from the Class-Conditioned UNet</h2>
      The sampling process is the same as part A, where we saw that conditional results aren't good unless we use classifier-free guidance. Use classifier-free guidance with $\gamma = 5.0$ for this part.

      <div style="text-align: center;">
        
        <img src="assets/algo4_c.png" alt="Algorithm Diagram"
        style="width: 30%; display: block; margin-left: auto; margin-right: auto" />
        <p class="text">Algorithm B.4. Sampling from class-conditioned UNet</p>
        
      </div>

      <!-- First row with 2 videos -->
      <div class="image-container"
        style="justify-content: center; max-width: 1200px; margin: 0 auto;">
        <div style="width: 100%; max-width: 600px;">
          
          <video id="video1" width="100%" muted loop playbackRate="0.75"
            style="display: block; margin-left: 0;">
            <source type="video/mp4" src="assets/new_c_1.mp4" />
          </video>
          <p style="text-align: left;">Epoch 1</p>
        </div>
        <div style="width: 100%; max-width: 600px;">
          
          <video id="video2" width="100%" muted loop playbackRate="0.75"
            style="display: block; margin-left: 0;">
            <source type="video/mp4" src="assets/new_c_5.mp4" />
          </video>
          <p style="text-align: left;">Epoch 5</p>
        </div>
      </div>

      <!-- Second row with 2 videos -->
      <div class="image-container"
        style="justify-content: center; max-width: 1200px; margin: 20px auto 0;">
        <div style="width: 100%; max-width: 600px;">
          
          <video id="video3" width="100%" muted loop playbackRate="0.75"
            style="display: block; margin-left: 0;">
            <source type="video/mp4" src="assets/new_c_10.mp4" />
          </video>
          <p style="text-align: left;">Epoch 10</p>
        </div>
        <div style="width: 100%; max-width: 600px;">
          
          <video id="video4" width="100%" muted loop playbackRate="0.75"
            style="display: block; margin-left: 0;">
            <source type="video/mp4" src="assets/new_c_15.mp4" />
          </video>
          <p style="text-align: left;">Epoch 15</p>
        </div>
      </div>

      <!-- Third row with 1 centered video -->
      <div class="image-container"
        style="justify-content: center; max-width: 1200px; margin: 20px auto 0;">
        <div style="width: 100%; max-width: 600px;">
          
          <video id="video5" width="100%" muted loop playbackRate="0.75"
            style="display: block; margin-left: 0;">
            <source type="video/mp4" src="assets/new_c_20.mp4" />
          </video>
          <p style="text-align: left;">Epoch 20</p>
        </div>
      </div>


      <h2>Deliverables</h2>
      <ul>
        <li>A training loss curve plot for the class-conditioned UNet over the whole training process.</li>
        <li>Sampling results for the class-conditioned UNet for 5 and 20 epochs. Generate 4 instances of each digit as shown above.
          <ul>
            <li>Note: providing a gif is optional and can be done as a bells and whistles below.</li>
          </ul>
        </li>
      </ul>

      
      <script>
  window.addEventListener('load', function() {
    // Handle video autoplay and playback speed for all videos
    var videos = document.querySelectorAll('video');
    videos.forEach(function(video) {
      video.playbackRate = 1;
      video.loop = false;
      video.play();

      // Add hover behavior to each video
      video.addEventListener('mouseover', function() {
        // Pause normal playback
        video.pause();
        
        // Play in reverse by decreasing currentTime
        const rewindInterval = setInterval(() => {
          if (video.currentTime <= 0) {
            clearInterval(rewindInterval);
          } else {
            video.currentTime -= 0.05; // Slow rewind speed
          }
        }, 40); // Smooth interval

        // Store the interval ID so we can clear it on mouseout
        video.rewindInterval = rewindInterval;
      });

      video.addEventListener('mouseout', function() {
        // Clear the rewind interval if it exists
        if (video.rewindInterval) {
          clearInterval(video.rewindInterval);
          video.rewindInterval = null;
        }
        // Play forward normally
        video.play();
      });
    });
  });

  // These handlers are no longer needed since we're handling everything in the load event
  function handleMouseOver(video) {
    // Empty or can be removed
  }

  function handleMouseOut(video) {
    // Empty or can be removed
  }
</script>
      <h2> Bells & Whistles </h2>
      <ul>
        <li><b>Sampling Gifs (.1 Cookie Points)</b><br>
          Create your own sampling gifs similar to the ones shown above.</li>

        <li><b>Improve the UNet Architecture for time-conditional generation (.15 Cookie Points)</b><br>
          For ease of explanation and implementation, our UNet architecture
          above is pretty simple.
          Modify the UNet (e.g. with skip connections) such that it can fit
          better during training and sample even better results.</li>

        <li><b>Implement Rectified Flow(.15 Cookie Points)</b>
          <ul>
            <li>Implement <a href="https://arxiv.org/abs/2209.03003">rectified
                flow</a>, which is the state of art diffusion model.</li>
            <li>You can reference any code on github, but your implementation
              needs to follow the same code structure as our DDPM
              implementation.</li>
            <li>In other words, the code change required should be minimal: only
              changing the forward and sample functions.</li>
          </ul>
        </li>

        <li><b>Your own ideas (N Cookie Points): be creative!</b></li>
      </ul>
      <h3>Acknowledgements</h3>
      <p>This project was a joint effort by <a
          href="https://dangeng.github.io/">Daniel Geng</a>, <a
          href="https://ryantabrizi.com/">Ryan Tabrizi</a>, and <a
          href="https://hangg7.com/">Hang Gao</a>, advised by <a
          href="https://liyueshen.engin.umich.edu/">Liyue Shen</a>, <a
          href="https://andrewowens.com/">Andrew Owens</a>, and <a
          href="https://people.eecs.berkeley.edu/~efros/">Alexei
          Efros</a>. </p>
    </body>
  </html>